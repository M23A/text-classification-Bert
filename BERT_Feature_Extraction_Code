import pandas as pd
import os
from transformers import BertTokenizer, BertModel
import torch
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define input and output paths
input_file = '/content/drive/MyDrive/Maram/Text/Text_chunks/cleaned_texts.csv'
output_dir = '/content/drive/MyDrive/Maram/Text/Features_Extraction_from_text/Features_chunk_Text/Bert/bert_features'
os.makedirs(output_dir, exist_ok=True)

# Load pre-trained BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

# Function to chunk text 
def chunk_text_by_tokens(text, tokenizer, max_tokens=512):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk = tokens[i:i + max_tokens]
        chunks.append(chunk)
    return chunks

# Function to get BERT embedding
def get_bert_embedding_from_tokens(tokens):
    inputs = tokenizer(tokens,return_tensors='pt',padding=True,truncation=True,max_length=512,is_split_into_words=True)
    with torch.no_grad():
        outputs = model(**inputs)
    return torch.mean(outputs.last_hidden_state, dim=1).squeeze().cpu().numpy()

# Load the single CSV
df = pd.read_csv(input_file)

# Lists to hold embeddings and IDs
all_embeddings = []
all_pids = []

# Process each row
for index, row in df.iterrows():
    text = str(row['text'])
    pid = row['filename']
    chunks = chunk_text_by_tokens(text, tokenizer)
    
    for token_chunk in chunks:
        embedding = get_bert_embedding_from_tokens(token_chunk)
        all_embeddings.append(embedding)
        all_pids.append(pid)


# Save as DataFrame
embeddings_df = pd.DataFrame(all_embeddings)
final_df = pd.concat([pd.Series(all_pids, name='PID'), embeddings_df], axis=1)
output_file = os.path.join(output_dir, "bert_features.csv")
final_df.to_csv(output_file, index=False)

print(f"Features saved to: {output_file}")
